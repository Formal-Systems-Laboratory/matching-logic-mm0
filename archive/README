This artifact includes the software used for generating proofs of equivalence
between and totality of regular expressions. It also includes testing
infrastructure for checking these proofs with Metamath Zero.

The benchmarks displayed in the paper are collected by running the `./test`
script. These may then be displayed in (pandoc) markdown using the
`./benchmark-aggregator.py` script. See `README` in archive for details.

These tests have been timed on the TACAS 23 VM running on a 11th Gen Intel(R)
Core(TM) i7-1165G7 host with 32GB of RAM. Default settings were used for
importing the VM into virtualbox, and NAT-based networking was used to install
dependencies.


Download the artifact
---------------------

The artifact is available at the citable URL:

    https://zenodo.org/records/10431211

Download the artifact using the following command:

    # Approx time: 1h
    $ wget https://zenodo.org/records/10431211/files/artifact.zip?download=1

unzip it using:

    $ unzip artifact.zip


Installing Third Party Dependecies
----------------------------------

This project depends on Metamath Zero for proof checking, and python and Maude for proof generation.
We also have some other python dependecies for testing and presenting benchmarks.
These are installed using poetry.
All of this is handled for the TACAS VM by running `./install-deps`.
This script needs `sudo` access, as well as internet access to bring in the
various dependencies.

Once this script completes run:

    $ PATH=/home/tacas23/.local/bin:$PATH

to place `poetry` in your shell.
If you open new shell or terminal this will need to be rerun.


Running Tests
-------------

Once this is done you may run the quick test, taking approximately 30 seconds.

    $ ./test --skip-slow

Running with omitting the `--skip-slow` flag runs the entire test suite,
taking approximately 26 minutes.

    $ ulimit -c unlimited   # Some tests need a deeper recursing depth
    $ ./test

Since flags to `test` are forwarded to `pytest`, we may view progress by
passing in the `-vv`.


Viewing Test Statistics
-----------------------

After a test run, you may view the statistics for each benchmark using:

    $ poetry run ./benchmark-aggregator.py

Here, the "`.mm1` time" is the proof generation time, in the human readable `mm1` format.
The "`.mm0` time" indicates the time to compile this to the binary `mm0` format.
The "check time" indicates the time to check this generated certificate.
Note that these are rounded down to the milisecond, so it may sometimes be zero.

The base time is the time for proving the handwritten lemmas.
All other statistics have the base statistics subtracted from them.


Adding New Tests
----------------

New tests may be added to `test.py`.
The easiest way to do this, is to add a new input to `test_regex`.
Note that this tests checks for validity (i.e. equivalence with the total language),
and not equivalence.
To check equivalence, use the `<<->>` operator.
Further, the regular expressions are parsed in Maude, and are a little sensitive to whitespace.
